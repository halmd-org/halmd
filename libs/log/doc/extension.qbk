[/
    This document is a part of Boost.Log library documentation.

    (c) 2008 Andrey Semashev

    Use, modification and distribution is subject to the Boost Software License, Version 1.0.
    (See accompanying file LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)
/]

[section:extension Extending the library]

[section:sinks Writing your own sinks]

    #include <boost/log/sinks/basic_sink_backend.hpp>

As was described in the [link log.design Design overview] section, sinks consist of two parts: frontend and backend. Frontends are provided by the library and usually need not to be re-implemented. Thanks to frontends, implementing backends is much easier than it could be: all filtering and thread synchronization is done there.

In order to develop a sink backend, you have two options where to start:
* If you don't need any formatting, the minimalistic `basic_sink_backend` base class template is your choice. Actually, this class only defines types that are needed for the sink to function.
* If you need to create a sink with formatting capabilities, you may use the `basic_formatting_sink_backend` class template as a base class for your backend. It extends the `basic_sink_backend` class and implements log record formatting and character code conversion, leaving you to only develop the record storing code.

Before we move on and see these instruments in action, one thing should be noted. As it was said before, sink frontends take the thread safety burden from the backend. Also, there are [link advanced.advanced.sink_frontends three types of frontends], each of them provides different guarantees regarding thread safety. The backend has no idea which sink frontend is used with it, yet it may require a certain degree of thread safety from it to function properly. In order to protect itself from misuse the backend declares the threading model it supports to operate with. There are three of them:

# The `backend_synchronization_tag` means that the backend itself is responsible for thread synchronization (which may imply there is no need for synchronization at all). When a backend declares this threading model, any sink frontend can be used with it.
# The `frontend_synchronization_tag` means that frontend must serialize calls to the backend from different threads. The `unlocked_sink` frontend cannot fulfill this requirement, so it will not compile if instantiated with such a backend.
# The `single_thread_tag` means that all log records must be passed to the backend in a single thread. Note that other methods can be called in other threads, however, these calls must be serialized. Only `asynchronous_sink` frontend meets this requirement, other frontends will refuse to compile with such a backend.

The threading model tag is used to instantiate the backend base classes. Since `basic_formatting_sink_backend` base class uses internal data to implement log record formatting, it requires the threading model to be either `frontend_synchronization_tag` or `single_thread_tag`. On the other hand, `basic_sink_backend` doesn't have this restriction.

[heading Minimalistic sink backend]

As an example of the `basic_sink_backend` class usage, let's implement a simple statistical information collector backend. Assume we have a network server and we want to monitor how many incoming connections are active and how much data was sent or received. The collected information should be written to a CSV-file every minute. The backend definition could look something like this:

    // The backend collects statistical information about network activity of the application
    class stat_collector :
        public sinks::basic_sink_backend<
            char,                               // Character type. We use narrow-character logging in this example.
            sinks::frontend_synchronization_tag // We will have to store internal data, so let's require frontend to
        >                                       // synchronize calls to the backend.
    {
    private:
        // The file to write the collected information to
        std::ofstream m_CSVFile;

        // Here goes the data collected so far:
        // Active connections
        unsigned int m_ActiveConnections;
        // Sent bytes
        unsigned int m_SentBytes;
        // Received bytes
        unsigned int m_ReceivedBytes;

        // A thread that writes the statistical information to the file
        std::auto_ptr< boost::thread > m_WriterThread;

    public:
        // The function creates an instance of the sink
        template< template< typename > class FrontendT >
        static boost::shared_ptr< FrontendT< stat_collector > > create(const char* file_name);

        // The function consumes the log records that come from the frontend
        void consume(values_view_type const& attributes, string_type const& message);

    private:
        // The constructor initializes the internal data
        explicit stat_collector(const char* file_name) :
            m_CSVFile(file_name, std::ios::app),
            m_ActiveConnections(0)
        {
            reset_accumulators();
            if (!m_CSVFile.is_open())
                throw std::runtime_error("could not open the CSV file");
        }
        // Destructor. Stops the file writing thread.
        ~stat_collector();

        // The function runs in a separate thread and calls write_data periodically
        template< template< typename > class FrontendT >
        static void writer_thread(boost::weak_ptr< FrontendT< stat_collector > > const& sink);

        // The function resets statistical accumulators to initial values
        void reset_accumulators()
        {
            m_SentBytes = m_ReceivedBytes = 0;
        }

        // The function writes the collected data to the file
        void write_data()
        {
            m_CSVFile << m_ActiveConnections << ',' << m_SentBytes << ',' << m_ReceivedBytes << std::endl;
            reset_accumulators();
        }
    };

As you can see, the public interface of the backend is quite simple. In fact, only the `consume` function is needed by frontends, the `create` function is introduced for our own convenience. The `create` function simply creates the sink and initializes the thread that will write the collected data to the file.

    // The function creates an instance of the sink
    template< template< typename > class FrontendT >
    boost::shared_ptr< FrontendT< stat_collector > > stat_collector::create(const char* file_name)
    {
        // Create the backend
        boost::shared_ptr< stat_collector > backend(new stat_collector(file_name));

        // Wrap it into the specified frontend
        boost::shared_ptr< FrontendT< stat_collector > > sink(new FrontendT< stat_collector >(backend));

        // Now we can start the thread that writes the data to the file
        backend->m_WriterThread.reset(new boost::thread(
            &stat_collector::writer_thread< FrontendT >,
            boost::weak_ptr< FrontendT< stat_collector > >(sink)
        ));

        return sink;
    }

Now the `writer_thread` function and destructor can look like this:

    // The function runs in a separate thread and writes the collected data to the file
    template< template< typename > class FrontendT >
    void stat_collector::writer_thread(boost::weak_ptr< FrontendT< stat_collector > > const& sink)
    {
        while (true)
        {
            // Sleep for one minute
            boost::this_thread::sleep(boost::get_xtime(
                boost::get_system_time() + boost::posix_time::minutes(1)));

            // Get the pointer to the sink
            boost::shared_ptr< FrontendT< stat_collector > > p = sink.lock();
            if (p)
                p->locked_backend()->write_data(); // write the collected data to the file
            else
                break; // the sink is dead, terminate the thread
        }
    }

    // Destructor. Stops the file writing thread.
    stat_collector::~stat_collector()
    {
        if (m_WriterThread.get())
        {
            m_WriterThread->interrupt();
            m_WriterThread->join();
        }
    }

The `consume` function is called every time the logging record passes filtering in the frontend. The record, as it was stated before, contains a set of attribute values (goes as the first argument of the function) and the message string (goes second). The types of these arguments are defined in the `basic_sink_backend` class.

Since we have no need in the record message, the second argument will not be of interest for us for now.

    // The function consumes the log records that come from the frontend
    void stat_collector::consume(values_view_type const& attributes, string_type const& message)
    {
        namespace bll = boost::lambda;

        if (attributes.count("Connected"))
            ++m_ActiveConnections;
        else if (attributes.count("Disconnected"))
            --m_ActiveConnections;
        else
        {
            logging::extract< unsigned int >("Sent", attributes, bll::var(m_SentBytes) += bll::_1);
            logging::extract< unsigned int >("Received", attributes, bll::var(m_ReceivedBytes) += bll::_1);
        }
    }

The code above is quite straightforward. We can parse through attribute values like through a regular map, or use extractors with functional objects to acquire individual values. __boost_lambda__ and similar libraries simplify generation of functional objects that will receive the extracted value.

[heading Formatting sink backend]

As an example of the formatting sink backend, let's implement a sink that will emit events to a Windows event trace. Assume there's another process that will receive these events and display them to the user in a balloon window near the notification area. The definition of such backend would look something like this:

    class event_notifier :
        public sinks::basic_formatting_sink_backend<
            char,    // the "source" character type
            wchar_t  // the "target" character type (optional, by default is the same as the source character type)
        >
    {
        // A handle for the event provider
        REGHANDLE m_ProviderHandle;

    public:
        // Constructor. Initializes the event source handle.
        explicit event_notifier(CLSID const& provider_id)
        {
            if (EventRegister(&provider_id, NULL, NULL, &m_ProviderHandle) != ERROR_SUCCESS)
                throw std::runtime_error("Could not register event provider");
        }
        // Destructor. Unregisters the event source.
        ~event_notifier()
        {
            EventUnregister(m_ProviderHandle);
        }

        // The method puts the formatted message to the event trace
        virtual void do_consume(values_view_type const& values, target_string_type const& formatted_message);
    };

The `basic_formatting_sink_backend` class template is instantiated on two character types: the one that is used by the rest of logging system and the one that is required by the backend for further usage. Either of these types can be `char` or `wchar_t`. These character types may be the same, in which case the formatting is done without character conversion, pretty much equivalent to streaming attribute values into a regular `std::ostringstream`. In our case the underlying API requires wide strings, so we'll have to do character conversion while formatting. The conversion will be done according to the locale that is set up in the `basic_formatting_sink_backend` base class (see `imbue` and `getloc` functions).

In order to differentiate the resulting string type from the string types used throughout the rest of logging library, the `basic_formatting_sink_backend` class defines the `target_string_type` type along with the standard `string_type`. In our case, `target_string_type` will contain wide characters, while `string_type` will be narrow.

The threading model of the sink backend can be specified as the third optional parameter of the `basic_formatting_sink_backend` class template. The default threading model is `frontend_synchronization_tag`, which fits us just fine.

The `basic_formatting_sink_backend` base class implements just about everything that is required by the library from the backend. The only thing left is to implement the virtual `do_consume` method that receives the set of attributes and the already formatted message. In our case this method will pass the formatted message to the corresponding API:

    // The method puts the formatted message to the event log
    void event_notifier::do_consume(values_view_type const& values, target_string_type const& formatted_message)
    {
        EventWriteString(m_ProviderHandle, WINEVENT_LEVEL_LOG_ALWAYS, 0ULL /* keyword */, formatted_message.c_str());
    }

That's it. The example can be extended to make use of attribute values to fill other parameters, like event level and keywords mask. A more elaborate version of this example can be found in the library examples.

The resulting sink backend can be used similarly to other formatting sinks, like `text_ostream_backend`:

    boost::shared_ptr< event_notifier > backend(new event_notifier(CLSID_MyNotifier));
    backend->set_formatter(fmt::ostrm << "[" << fmt::time("TimeStamp") << "] " << fmt::message());

    boost::shared_ptr< sinks::synchronous_sink< event_notifier > > sink(new sinks::synchronous_sink< event_notifier >(backend));
    logging::core::get()->add_sink(sink);

[endsect]

[section:sources Writing your own sources]

    #include <boost/log/sources/threading_models.hpp>
    #include <boost/log/sources/basic_logger.hpp>

You can extend the library by developing your own sources and, for that matter, ways of collecting log data. Basically, you have two choices how to start: you can either develop a new logger feature or design a whole new type of source. The first approach is good if all you need is to tweak functionality of the existing loggers. The second approach is reasonable if the whole mechanism of collecting logs by the provided loggers is unsuitable for your needs.

[heading Creating a new logger feature]

Every logger provided by the library consists of a number of features that can be combined with each other. Each feature is responsible for a single and independent aspect of the logger functionality. For example, loggers that provide the ability to assign severity levels to logging records include the `basic_severity_logger` feature. You can implement your own feature and use it along with the ones provided by the library.

A logger feature should follow these basic requirements:

* A logging feature should be a class template. It should have at least one template parameter type (let's name it `BaseT`).
* The feature must publicly derive from the `BaseT` template parameter.
* The feature must be default-constructible and copy-constructible.
* The feature must be constructible with a single argument of a templated type. The feature may not use this argument itself, but it should pass this argument to the `BaseT` constructor.

These requirements allow to compose a logger from a number of features derived from each other. The root class of the features hierarchy will be the `basic_logger` class template instance. This class implements the most of the basic functionality of loggers, like storing logger-specific attributes and providing interface for log message formatting. The hierarchy composition is done by the `basic_composite_logger` class template, which is instantiated on an MPL sequence of features (don't worry, this will be shown in an example in a few moments). The constructor with a templated argument allows to initialize features with named parameters, using the __boost_parameter__ library.

A logging feature may also contain internal data. In that case, to maintain thread safety for the logger, the feature should follow these additional guidelines:

# Usually there is no need to introduce a mutex or another synchronization mechanism in each feature. Moreover, it is advised not to do so, because the same feature can be used in both thread-safe and not thread-safe loggers. Instead, features should use threading model of the logger as a synchronization primitive, similarly as they would use mutex. The threading model is accessible through the `threading_base` method, defined in the `basic_logger` class template.
# If the feature has to override methods of the public interface of the `basic_logger` class template (or the same part of the base feature interface), the following should be considered with regard to such methods:

    * The public interface of the feature should be thread-safe in terms of its own thread safety requirements and its base classes requirements.
    * The thread safety requirements are expressed with lock types for each public function the feature exposes to the user. These types are available as typedefs in each feature. If the feature exposes a public function `foo`, it will also expose type `foo_lock`, which will express the locking requirements of `foo`. Feature constructors don't need locking, and thus there's no need for lock types for them.
    * The feature should also provide a protected interface for the derived features. This interface should provide operations equivalent to the public interface, but should use no locking. If the feature exposes a public function `foo`, it will also expose a protected function `foo_unlocked` with the same meaning.
    * The feature should not call public methods of the base class interface when the threading model is already locked. Instead, the *`_unlocked` versions of functions should be called.

# The feature may implement copy constructor. The argument of the constructor is already locked with a shared lock when the constructor is called. Naturally, the feature is expected to forward copy constructor call to the `BaseT` class.
# The feature need not implement assignment operator. The assignment will be automatically provided by the `basic_composite_logger` class instance. However, the feature may provide `swap_unlocked` method that will swap contents of this feature and the method argument, and call similar method in the `BaseT` class. The automatically generated assignment operator will use this method, along with copy constructor.

In order to illustrate all these lengthy recommendations, let's implement a simple logger feature. Suppose we want our logger to be able to tag individual log records. In other words, the logger has to temporarily add an attribute to its set of attributes, emit the logging record, and then automatically remove the attribute. Somewhat similar functionality can be achieved with scoped attributes, although the syntax may complicate wrapping it into a neat macro:

    // We want something equivalent to this
    {
        BOOST_LOG_SCOPED_LOGGER_TAG(logger, "Tag", std::string, "[GUI]");
        BOOST_LOG(logger) << "The user has confirmed his choice";
    }

Let's declare our logger feature:

    template< typename BaseT >
    class record_tagger :
        public BaseT  // the feature should derive from other features or the basic_logger class
    {
    public:
        // Let's import some types that we will need. These imports should be public,
        // in order to allow other features that may derive from record_tagger to do the same.
        typedef typename BaseT::string_type string_type;
        typedef typename BaseT::attribute_set_type attribute_set_type;
        typedef typename BaseT::threading_model threading_model;

    private:
        // The iterator that points to the temporary tag attribute
        typename attribute_set_type::iterator m_Tag;

    public:
        // Default constructor. Initializes m_Tag to an invalid value.
        record_tagger();
        // Copy constructor. Initializes m_Tag to a value, equivalent to that.m_Tag.
        record_tagger(record_tagger const& that);
        // Forwarding constructor with named parameters
        template< typename ArgsT >
        record_tagger(ArgsT const& args);

        // Import the method without arguments from the base class
        using BaseT::open_record;

        // The method overrides the same named method in the base class.
        // It initiates a new log record, performs filtering and returns the result of filtering.
        // It also accepts a number of named parameters, that we will use
        // to pass our tag.
        template< typename ArgsT >
        bool open_record(ArgsT const& args);

        // The method will require locking, so we have to define locking requirements for it.
        // We use the strictest_lock trait in order to choose the most restricting lock type.
        typedef typename src::strictest_lock<
            boost::lock_guard< threading_model >,
            typename BaseT::open_record_lock,
            typename BaseT::add_attribute_lock,
            typename BaseT::remove_attribute_lock
        >::type open_record_lock;

        // Similarly, declare methods of logging record finalization
        void push_record(string_type const& message);
        typedef typename src::strictest_lock<
            boost::lock_guard< threading_model >,
            typename BaseT::push_record_lock,
            typename BaseT::remove_attribute_lock
        >::type push_record_lock;

        void cancel_record();
        typedef typename src::strictest_lock<
            boost::lock_guard< threading_model >,
            typename BaseT::cancel_record_lock,
            typename BaseT::remove_attribute_lock
        >::type cancel_record_lock;

    protected:
        // Import the method without arguments from the base class
        using BaseT::open_record_unlocked;

        // Lock-less implementation of operations
        template< typename ArgsT >
        bool open_record_unlocked(ArgsT const& args);
        void push_record_unlocked(string_type const& message);
        void cancel_record_unlocked();

        // This function will be needed by the assignment operator of the logger
        void swap_unlocked(record_tagger& that);

    private:
        void remove_tag();

    public:
        // In order to simplify feature composition, we make this class capable
        // to participate in MPL Lambda expressions. On most compilers this line
        // will expand into nothingness.
        BOOST_MPL_AUX_LAMBDA_SUPPORT(1, record_tagger, (BaseT))
    };

You can see that we use the `strictest_lock` template in order to define lock types that would fulfill the base class thread safety requirements for methods that are to be called from the corresponding methods of `record_tagger`. For example, the `open_record_lock` definition shows that the `open_record` implementation in the `record_tagger` feature requires exclusive lock (which `lock_guard` is) for the logger, but it also takes into account locking requirements of the `open_record`, `add_attribute` and `remove_attribute` methods of the base class, because it will have to call them.

Frankly speaking, in this particular example, there was no need in using the `strictest_lock` trait, because all our methods require exclusive locking, which is already the strictest one. However, this template may come handy if you use shared locking.

Now the implementation of the public interface becomes quite simple:

    record_tagger::record_tagger() : m_Tag(BaseT::attributes().end())
    {
    }

    record_tagger::record_tagger(record_tagger const& that) :
        BaseT(static_cast< BaseT const& >(that)),
        m_Tag(BaseT::attributes().end()) // the tag cannot be set during copying the logger
    {
    }

    template< typename ArgsT >
    record_tagger::record_tagger(ArgsT const& args) : BaseT(args)
    {
    }

    template< typename ArgsT >
    bool record_tagger::open_record(ArgsT const& args)
    {
        // Acquire the lock and forward to the unlocked implementation
        open_record_lock lock(this->threading_base());
        return open_record_unlocked(args);
    }

    void record_tagger::push_record(string_type const& message)
    {
        push_record_lock lock(this->threading_base());
        push_record_unlocked(message);
    }

    void record_tagger::cancel_record()
    {
        cancel_record_lock lock(this->threading_base());
        cancel_record_unlocked();
    }

Note that the locks we have defined in the feature definition are applied to the threading model. This will allow the compiler to effectively optimize locking away in case of a single-threaded model.

Now, when all locking is extracted into the public interface, we have the most of our feature logic to be implemented in the protected part of the interface. In order to set up tag value in the logger, we will have to introduce a new __boost_parameter__ keyword. Following recommendations from that library documentation, it's better to introduce the keyword in a special namespace:

    namespace my_keywords {

        BOOST_PARAMETER_KEYWORD(tag_ns, tag)

    }

Opening a new record can now look something like this:

    template< typename ArgsT >
    bool record_tagger::open_record_unlocked(ArgsT const& args)
    {
        // Extract the named argument from the parameters pack
        string_type tag_value = args[my_keywords::tag | string_type()];

        if (!tag_value.empty())
        {
            // Add the tag as a new attribute
            boost::shared_ptr< logging::attribute > attr(
                new logging::constant< string_type >(tag_value));
            std::pair<
                typename attribute_set_type::iterator,
                bool
            > res = BaseT::add_attribute_unlocked("Tag", attr);
            if (res.second)
                m_Tag = res.first;
        }

        // And forward the call to the base features
        bool opened = BaseT::open_record_unlocked(args);

        // If the record did not pass filtering, remove the tag
        if (!opened)
            remove_tag();

        return opened;
    }

If a log record passes filtering, either `push_record` or `cancel_record` gets called. Here are their implementation:

    void record_tagger::push_record_unlocked(string_type const& message)
    {
        // Push the record as usual
        BaseT::push_record_unlocked(message);

        // Remove the tag from the set of attributes
        remove_tag();
    }

    void record_tagger::cancel_record_unlocked()
    {
        // Cancel the record as usual
        BaseT::cancel_record_unlocked();

        // Remove the tag from the set of attributes
        remove_tag();
    }

    void record_tagger::remove_tag()
    {
        if (m_Tag != BaseT::attributes().end())
        {
            BaseT::remove_attribute_unlocked(m_Tag);
            m_Tag = BaseT::attributes().end();
        }
    }

The last unimplemented method that is left is `swap_unlocked`. This function is somewhat special, because it doesn't need the public counterpart with locking. The public `swap` method will be generated automatically by the composite logger class, and thus it will override any such method in the feature class. Additionally, the composite logger will implement assignment operator that will be using copy constructor and this swap method.

    void record_tagger::swap_unlocked(record_tagger& that)
    {
        using std::swap;
        swap(m_Tag, that.m_Tag);
        BaseT::swap_unlocked(static_cast< BaseT& >(that));
    }

Ok, we got our feature, and it's time to inject it into a logger. Assume we want to combine it with the standard severity level logging. No problems:

    namespace mpl = boost::mpl;

    template< typename LevelT = int >
    class my_logger :
        public src::basic_composite_logger<
            char,                       // character type for the logger
            my_logger,                  // final logger type
            src::single_thread_model,   // the logger does not perform thread synchronization
                                        // use multi_thread_model to declare a thread-safe logger
            typename mpl::vector<       // the list of features we want to combine
                src::basic_severity_logger< mpl::_1, LevelT >,
                record_tagger< mpl::_1 >
            >::type
        >
    {
        // The following line will automatically generate forwarding constructors that
        // will call to the corresponding constructors of the base class
        BOOST_LOG_FORWARD_LOGGER_CONSTRUCTORS_TEMPLATE(my_logger)
    };

As you can see, creating a logger is a quite simple procedure. The `BOOST_LOG_FORWARD_LOGGER_CONSTRUCTORS_TEMPLATE` macro you see here is for mere convenience purpose: it unfolds into a default constructor, copy constructor and a number of constructors to support named arguments. For non-template loggers there is a similar `BOOST_LOG_FORWARD_LOGGER_CONSTRUCTORS` macro.

To use this logger we can now write the following:

    enum severity_level
    {
        normal,
        warning,
        error
    };

    my_logger< severity_level > logger;

    if (logger.open_record((keywords::severity = normal, my_keywords::tag = "[GUI]")))
        logger.strm() << "The user has confirmed his choice";

However, I would prefer defining a special macro to reduce verbosity:

    #define LOG_WITH_TAG(lg, sev, tg) \
        BOOST_LOG_WITH_PARAMS((lg), (keywords::severity = (sev))(my_keywords::tag = (tg)))

    LOG_WITH_TAG(logger, normal, "[GUI]") << "The user has confirmed his choice";

[heading Guidelines for the complete logging source designers]

In general, you can implement new logging sources the way you like, the library does not mandate any design requirements on log sources. However, there are some notes regarding the way log sources should interact with logging core.

# Whenever a logging source is ready to emit a log record, it should call the `open_record` in the corresponding core. The source-specific attributes should be passed into that call. During that call the core allocates resources for the record being made and performs filtering.
# If the call to `open_record` returned `true`, then the record passed the filtering and is considered to be opened. The record must be either confirmed by the source by subsequently calling `push_record` or withdrawned by calling to `cancel_record`. In either case, after the call the resources associated with the record will be reclaimed (as far as the logging core is concerned, that is). The behavior is undefined if either of these methods is called when no record is opened.
# If the call to `open_record` returned `false`, it means that the record has not been opened (most likely due to filtering reject). In that case the logging core does not hold any resources associated with the record, and thus the source must not call `push_record` or `cancel_record` for that particular logging attempt.
# The source may subsequently open more than one records. In that case the state of each record is pushed into an internal stack, so the source must confirm or withdraw the opened records in the reverse order to their opening.
# The core does not distinguish sources from each other. That means that one source can open a record, then second source can open a record. If the first source closes the record either way, it will close the record that was opened last (the one that was opened by the second source in this example). The core, however, distinguishes between threads in which the log records are manipulated, so that different threads do not interfere with each other.
# The cores for different character types are completely independent.

[endsect]

[section:attributes Writing your own attributes]

    #include <boost/log/attributes/attribute.hpp>
    #include <boost/log/attributes/basic_attribute_value.hpp>

Developing your own attributes is quite simple. Generally, you need to do the following:

# Define what will be the attribute value. Most likely, it will be a piece of constant data that you want to participate in filtering and formatting. Encapsulate this data into a class that derives from the `attribute_value` interface. This object will have to implement the `dispatch` method that will extract the stored data (or, in other words, the stored value) to a type dispatcher.
# Define how attribute values are going to be produced. In a corner case the values need not to be produced (like in case of the `constant` attribute provided by the library), but often there is some logic that need to be invoked to acquire the attribute value. This logic has to be concentrated in a class derived from the `attribute` interface, more precisely - in the `get_value` method. You may think of it as an attribute value factory.

While designing an attribute, one has to strive for making it as independent from values it produces, as possible. The attribute can be called from different threads concurrently to produce a value. Once produced, the attribute value can be used several times by the library (maybe even concurrently), it can outlive the attribute object where it was created, and there can simultaneously exist several attribute values produced by the same attribute.

Each attribute value is considered independent from other attribute values or the attribute itself, from the point of view of the library. That said, it is still possible to implement attributes that are also attribute values, and thus optimize performance. This is possible if either of these fulfills:

* The attribute value never changes, so it's possible to store it into the attribute itself. The `constant` attribute is an example.
* The attribute stores its value in a global (external with regard to the attribute) storage, that can be accessed from any attribute value. The attribute values must guarantee, though, that their stored values do not change over time.

As a special case for the second point, it is possible to store attribute values (or their parts) in a thread-specific storage. However, in that case user has to implement the `detach_from_thread` method of the attribute value properly. The result of this method - another attribute value - must be independent from the thread is is being called in, but its stored value should be equivalent to the original attribute value. This method will be called by the library when the attribute value passes to thread that is different from the thread where it was created. As of this moment, this will only happen in case of asynchronous logging sink.

But in vast majority of cases attribute values must be self-contained objects with no dependencies on other entities. In fact, this case is so common that the library provides a ready to use attribute value class template `basic_attribute_value`. The template has to be instantiated on the stored value type, and the stored value has to be provided to the attribute value constructor. For example, this is how to implement an attribute that will return system uptime in seconds:

    class system_uptime :
        public logging::attribute
    {
        typedef attrs::basic_attribute_value< unsigned int > attribute_value_type;

    public:
        boost::shared_ptr< attribute_value > get_value()
        {
            unsigned int up;

    #if defined(BOOST_WINDOWS)
            up = GetTickCount() / 1000;
    #else // assume other platforms provide sysinfo function
            struct sysinfo info;
            if (sysinfo(&info) != 0)
                throw std::runtime_error("Could not acquire uptime");
            up = info.uptime;
    #endif

            return boost::shared_ptr< attribute_value >(new attribute_value_type(up));
        }
    };

[endsect]

[section:settings Extending library settings support]

If you write your own logging sinks or use your own types in attributes, you may want to add support for these components to the settings parser provided by the library. Without doing this, the library will not be aware of your types and thus will not work properly.

[heading Adding support for user-defined types to the formatter parser]

    #include <boost/log/utility/init/formatter_parser.hpp>

In order to add support for user-defined types to the formatter parser, one has to register a formatter factory. The factory is basically a functional object that, upon calling, will construct a formatter for the particular attribute. Factories are registered with the `register_formatter_factory` function, that besides the factory functor accepts the attribute name that will trigger this factory usage. This way the application can expose the knowledge of the particular attribute nature to the library. Here's a quick example:

    // Suppose, this class can be used as an attribute value
    struct Point
    {
        double m_X, m_Y;

        // Streaming operator
        template< typename CharT, typename TraitsT >
        friend std::basic_ostream< CharT, TraitsT >& operator<< (
            std::basic_ostream< CharT, TraitsT >& strm, Point const& point)
        {
            strm << "(" << point.m_X << ", " << point.m_Y << ")";
        }
    };

    // This is a helper traits that defines most of the types used by the formatter factories
    typedef logging::formatter_types< char > types;

    // Formatter factory
    types::formatter_type point_formatter_factory(
        types::string_type const& attr_name,
        types::formatter_factory_args const& args)
    {
        return types::formatter_type(fmt::attr< Point >(attr_name));
    }

    // We can associate the attribute with name "Coordinates" with the type Point
    logging::register_formatter_factory("Coordinates", &point_formatter_factory);

Now, whenever the formatter parser (the `parse_formatter` function) encounters the "Coordinates" attribute in the format string being parsed, the `point_formatter_factory` will be called to construct the appropriate formatter. This formatter, since it is generated in the user's application, will use the custom streaming operator that is defined for the `Point` class.

The formatter factory can additionally accept a number of parameters separated with commas that can be specified in the format string. These parameters are broken into (name, value) pairs and passed as the second argument to the factory. For example, we could allow to customize the way our coordinates are presented in log by accepting an additional parameter in the format string like this:

[pre %TimeStamp% %Coordinates(format\="{%0.3f; %0.3f}")% %\_%]

Now in order to support this parameter we should rewrite our factory like this:

    namespace bll = boost::lambda;

    // This formatter will use custom format string to format the point coordinates
    void custom_point_formatter(
        types::string_type const& attr_name,
        types::ostream_type& strm,
        types::values_view_type const& attrs,
        types::string_type const& format)
    {
        Point point;
        if (logging::extract< Point >(attr_name, attrs, bll::var(point) = bll::_1))
        {
            // If the attribute set contains the needed attribute value, format it into the stream
            strm << boost::format(format) % point.m_X % point.m_Y;
        }
    }

    // Formatter factory
    types::formatter_type point_formatter_factory(
        types::string_type const& attr_name,
        types::formatter_factory_args const& args)
    {
        types::formatter_factory_args::const_iterator it = args.find("format");
        if (it != args.end())
        {
            // The custom format is specified, use the special formatter
            return types::formatter_type(bll::bind(&custom_point_formatter, attr_name, bll::_1, bll::_2, it->second));
        }
        else
        {
            // No special format specified, do things the traditional way
            return types::formatter_type(fmt::attr< Point >(attr_name));
        }
    }

However, if you don't need this additional flexibility and all you want is to use your custom streaming operators to format the attribute value, you can omit writing the formatter factory altogether. You can use a simple call like this:

    logging::register_simple_formatter_factory< Point >("Coordinates");

to achieve the same effect that the first version of the `point_formatter_factory` function provides.

[heading Adding support for user-defined types to the filter parser]

    #include <boost/log/utility/init/filter_parser.hpp>

You can extend filter parser the similar way you can extend the formatter parser - by registering your types into the library. However, since it takes a considerably more complex syntax to describe filters, filter factories are more than a mere function.

Filter factories should be objects that derive from the `filter_factory` interface. This base class declares a number of virtual functions that will be called in order to create filters, according to the filter expression. If some functions are not overriden by the factory, the corresponding operations are considered to be not supported by the attribute value. For example, we can define the filter factory for the slightly improved `Point` class defined in the previous section the following way:

    // Suppose, this class can be used as an attribute value
    struct Point
    {
        double m_X, m_Y;

        // Comparison operators
        bool operator== (Point const& that) const;
        bool operator!= (Point const& that) const;

        // Streaming operators
        template< typename CharT, typename TraitsT >
        friend std::basic_ostream< CharT, TraitsT >& operator<< (
            std::basic_ostream< CharT, TraitsT >& strm, Point const& point);
        template< typename CharT, typename TraitsT >
        friend std::basic_istream< CharT, TraitsT >& operator>> (
            std::basic_istream< CharT, TraitsT >& strm, Point& point);
    };

    struct point_filter_factory :
        public logging::filter_factory< char >
    {
        // The callback for filter for the attribute existence test
        filter_type on_exists_test(string_type const& name)
        {
            return filter_type(flt::has_attr< Point >(name));
        }

        // The callback for equality relation filter
        filter_type on_equality_relation(string_type const& name, string_type const& arg)
        {
            return filter_type(flt::attr< Point >(name) == boost::lexical_cast< Point >(arg));
        }
        // The callback for inequality relation filter
        filter_type on_inequality_relation(string_type const& name, string_type const& arg)
        {
            return filter_type(flt::attr< Point >(name) != boost::lexical_cast< Point >(arg));
        }
    };

    // The factory can be registered the following way
    logging::register_filter_factory("Coordinates", boost::make_shared< point_filter_factory >());

Having done that, whenever the filter parser (the `parse_filter` function) encounters the "Coordinates" attribute mentioned in the filter, it will use the `point_filter_factory` object to construct the appropriate filter. For example, in case of filter

[pre %Coordinates% \= "(10, 10)"]

the `on_equality_relation` method will be called with `name` argument being "Coordinates" and `arg` being "10, 10".

[note The quotes around the parenthesis are necessary because the filter parser only supports binary relations, while round brackets are already used to group subexpressions of the filter expression. Whenever there is need to pass several parameters to the relation (like in this case - a number of components of the `Point` class) the parameters should be encoded into a quoted string. The string may include C-style escape sequences that will be unfolded upon parsing.]

The constructed filter will use the corresponding comparison operators for the `Point` class. Some relation operations, like ">" or "<=", will not be supported for attributes named "Coordinates", and this is just the way we want it, because the `Point` class does not support them either.

The library allows not only to add support for new types, but also to associate new relations with them. For instance, we can create a new relation "is_in_rect" that will yield positive if the coordinates fit into a rectangle denoted with two points. The filter might look like this:

[pre %Coordinates% is\_in\_rect "(10, 10) - (20, 20)"]

To support it one has to define the `on_custom_relation` method in the filter factory:

    namespace bll = boost::lambda;

    struct Rectangle
    {
        Point m_TopLeft, m_BottomRight;

        // Streaming operators
        template< typename CharT, typename TraitsT >
        friend std::basic_ostream< CharT, TraitsT >& operator<< (
            std::basic_ostream< CharT, TraitsT >& strm, Rectangle const& rect);
        template< typename CharT, typename TraitsT >
        friend std::basic_istream< CharT, TraitsT >& operator>> (
            std::basic_istream< CharT, TraitsT >& strm, Rectangle& rect);
    };

    // Our custom filter type
    class is_in_rect_filter :
        public flt::basic_filter< char, is_in_rect_filter >
    {
    private:
        string_type m_Name;
        Rectangle m_Rect;

    public:
        is_in_rect_filter(string_type const& attr_name, Rectangle const& rect) : m_Name(attr_name), m_Rect(rect)
        {
        }

        bool operator() (values_view_type const& attrs) const
        {
            Point point;
            if (logging::extract< Point >(m_Name, attrs, bll::var(point) = bll::_1))
            {
                // Check that the point fits into the rectangle region
                return point.m_X >= m_Rect.m_TopLeft.m_X && point.m_X <= m_Rect.m_BottomRight.m_X
                    && point.m_Y >= m_Rect.m_TopLeft.m_Y && point.m_Y <= m_Rect.m_BottomRight.m_Y;
            }
            else
                return false;
        }
    };

    struct point_filter_factory :
        public logging::filter_factory< char >
    {
        // The callback for custom relation filter
        filter_type on_custom_relation(string_type const& name, string_type const& rel, string_type const& arg)
        {
            if (rel == "is_in_rect")
            {
                // Parse the coordinates of the rectangle region and construct the filter
                return filter_type(is_in_rect_filter(name, boost::lexical_cast< Rectangle >(arg));
            }
            else
                throw std::runtime_error("Relation " + rel + " is not supported");
        }
    };

Like with formatters, if all these bells and whistles are not needed, user can register a trivial filter factory with a simple call:

    logging::register_simple_filter_factory< Point >("Coordinates");

In this case, however, the `Point` class has to support all standard relational operations and have appropriate streaming operators in order to be parsed from a string.

[heading Adding support for user-defined sinks]

    #include <boost/log/utility/init/from_stream.hpp>

The library provides mechanism of extending support for sinks similar to the formatter and filter parsers. In order to be able to mention user-defined sinks in a settings file, the user has to register a sink factory, which is essentially a functional object that receives a number of named parameters and returns a pointer to the initialized sink. The factory is registered for a specific destination (see the [link advanced.advanced.utilities.init.settings settings file description]), so whenever a sink with the specified destination is mentioned in the settings file, the factory gets called. For instance, if we have a sink that emits SNMP traps as a result of processing log records, we can register it the following way:

    class snmp_backend :
        public sinks::basic_sink_backend< char, sinks::frontend_synchronization_tag >
    {
    public:
        // The constructor takes an address of the receiver of the traps
        explicit snmp_backend(std::string const& trap_receiver);

        // The function consumes the log records that come from the frontend and emits SNMP traps
        void consume(values_view_type const& attributes, string_type const& message);
    };

    // Factory function for the SNMP sink
    boost::shared_ptr< sinks::sink< char > > create_snmp_sink(std::map< std::string, std::string > const& params)
    {
        // Read parameters for the backend and create it
        std::map< std::string, std::string >::const_iterator it = params.find("TrapReceiver");
        if (it == params.end())
            throw std::runtime_error("TrapReceiver parameter not specified for the SNMP backend");

        boost::shared_ptr< snmp_backend > backend = boost::make_shared< snmp_backend >(it->second);

        // Construct and initialize the final sink
        boost::shared_ptr< sinks::synchronous_sink< snmp_backend > > sink =
            boost::make_shared< sinks::synchronous_sink< snmp_backend > >(backend);

        it = params.find("Filter");
        if (it != params.end())
            sink->set_filter(logging::parse_filter(it->second));

        return sink;
    }

    logging::register_sink_factory("SNMP", &create_snmp_sink);

Now the SNMP sink can be constructed with the following settings:

[pre
\[Sink:MySNMPSink\]

Destination\=SNMP
Filter\="%Severity% > 3"
]

[tip Although users are free to name parameters of their sinks the way they like, a good choice would be to follow the naming policy established by the library. That is, it should be obvious that the parameter "Filter" means the same for both the library-provided "TextFile" sink and your custom "SNMP" sink backend.]

[note As the "Destination" parameter is used to determine the sink factory, this parameter is reserved and cannot be used by sink factories for their purpose.]

[endsect]

[endsect]
